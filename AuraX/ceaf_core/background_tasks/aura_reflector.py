# Em: ceaf_core/background_tasks/aura_reflector.py

import logging
import random
import re
from datetime import datetime
import json

import numpy as np
from sklearn.cluster import DBSCAN
from pathlib import Path
from typing import List, Dict, Any, Optional
import httpx

from agent_manager import AgentManager
from database.models import AgentRepository
from .kg_processor import KGProcessor
from ceaf_core.modules.memory_blossom.memory_types import (
    GoalRecord,
    GoalStatus,
    ExplicitMemory,
    ExplicitMemoryContent,
    MemorySourceType,
    MemorySalience,
    GenerativeMemory,
    GenerativeSeed
)



from ceaf_core.services.cognitive_log_service import CognitiveLogService
from ceaf_core.system import CEAFSystem
from ceaf_core.modules.memory_blossom.advanced_synthesizer import AdvancedMemorySynthesizer, StoryArcType
from ceaf_core.services.llm_service import LLMService
from ceaf_core.system import CeafSelfRepresentation
from ..modules.ncim_engine.ncim_module import LLM_MODEL_FOR_REFLECTION
from ..utils import extract_json_from_text

logger = logging.getLogger("AuraReflector")

CONFIDENCE_THRESHOLD_FOR_SUCCESS = 0.75
MIN_TURNS_FOR_ANALYSIS = 500
PROACTIVITY_ACTIVATION_THRESHOLD = 0.45
AURA_API_BASE_URL = "http://127.0.0.1:8009/ceaf"


async def perform_topological_consolidation(self, agent_id: str, agent_manager: AgentManager):
    """
    PASSO 7 (B): Consolida√ß√£o de Identidade e Conhecimento via TDA.
    """
    agent_instance = agent_manager.get_agent_instance(agent_id)
    tda_engine = agent_instance.monitor.tda_engine  # Acessa o motor de persist√™ncia

    # 1. Coleta todas as mem√≥rias do MBS
    memories = await agent_instance.memory_service.search_raw_memories(query="*", top_k=100)
    embeddings = [agent_instance.memory_service._embedding_cache[m[0].memory_id] for m in memories]

    # 2. Calcula a entropia do 'C√©rebro' de longo prazo
    brain_ths = tda_engine.calculate_ths(embeddings)

    if brain_ths > 0.7:
        logger.warning(f"üß† {agent_id}: C√©rebro fragmentado (THS={brain_ths:.2f}). Gerando meta-insight.")
        # Se a entropia for alta, for√ßamos o BiRAG a fundir mem√≥rias dispersas
        # em um √∫nico 'Atrator de Resumo'.

async def calibrate_homeostasis(self, agent_id: str, rollback_count: int):
    """
    PASSO 7: Calibra√ß√£o de Homeostase via Reinforcement Learning Interno.
    Se o sistema est√° tendo muitos rollbacks, ele est√° 'ansioso' (ca√≥tico).
    """
    profile = self.agent_manager.get_agent_profile(agent_id)

    if rollback_count > 5:
        logger.critical(f"üß† Calibrando Homeostase para {agent_id}: Aumentando Rigidez.")
        # Aumentamos o baseline_coherence_bias para evitar rollbacks futuros
        profile.mcl_config.baseline_coherence_bias += 0.05
        self.agent_manager.update_agent_profile(agent_id, profile.model_dump())

async def evolve_identity(self, validated_insight_vector: List[float]):
    """Fus√£o Fractal: A persona evolui 5% em dire√ß√£o ao novo aprendizado."""
    new_glyph = (0.95 * np.array(self.glyph_g)) + (0.05 * np.array(validated_insight_vector))
    self.glyph_g = new_glyph.tolist()
    logger.info("üß¨ Identidade evolu√≠da suavemente.")

def analyze_correlation_guidance_confidence(turn_history: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Analisa a correla√ß√£o entre os biases de orienta√ß√£o (coer√™ncia vs. novidade) do MCL
    e a confian√ßa da resposta final, para descobrir se o agente est√° sendo muito
    "ca√≥tico" ou muito "r√≠gido".
    """
    results = {
        "coherence_leaning_success_rate": 0.0,
        "novelty_leaning_success_rate": 0.0,
        "coherence_turn_count": 0,
        "novelty_turn_count": 0,
        "suggestion": "insufficient_data"
    }

    coherence_successes = 0
    novelty_successes = 0

    for turn in turn_history:
        try:
            # Pega a orienta√ß√£o completa do MCL que foi salva no log
            mcl_guidance = turn.get("mcl_guidance")
            if not mcl_guidance:
                continue  # Pula turnos que n√£o t√™m o log de orienta√ß√£o

            # Extrai os biases que foram REALMENTE usados naquele turno
            biases = mcl_guidance.get("biases")
            if not biases:
                continue

            coherence_bias = biases.get("coherence_bias", 0.5)
            novelty_bias = biases.get("novelty_bias", 0.5)

            # Verifica se o resultado foi um "sucesso" (confian√ßa alta)
            is_successful = turn["response_packet"]["confidence_score"] > CONFIDENCE_THRESHOLD_FOR_SUCCESS

            # Classifica o turno como orientado a coer√™ncia ou novidade e conta os sucessos
            if coherence_bias > novelty_bias:
                results["coherence_turn_count"] += 1
                if is_successful:
                    coherence_successes += 1
            elif novelty_bias > coherence_bias:  # Usamos elif para ignorar o caso de empate
                results["novelty_turn_count"] += 1
                if is_successful:
                    novelty_successes += 1
        except (KeyError, IndexError, TypeError) as e:
            logger.warning(f"AuraReflector: Pulando turno malformado durante an√°lise de correla√ß√£o: {e}")
            continue

    # Calcula as taxas de sucesso se houver dados suficientes
    if results["coherence_turn_count"] > 5:
        results["coherence_leaning_success_rate"] = coherence_successes / results["coherence_turn_count"]

    if results["novelty_turn_count"] > 5:
        results["novelty_leaning_success_rate"] = novelty_successes / results["novelty_turn_count"]

    # Gera uma sugest√£o de ajuste com base na compara√ß√£o das taxas de sucesso
    coh_rate = results["coherence_leaning_success_rate"]
    nov_rate = results["novelty_leaning_success_rate"]

    # Apenas sugere uma mudan√ßa se houver dados para ambos os tipos de orienta√ß√£o e uma diferen√ßa significativa
    if coh_rate > 0 and nov_rate > 0:
        if coh_rate > nov_rate + 0.15:  # Coer√™ncia √© 15% mais bem-sucedida
            results["suggestion"] = "increase_coherence_bias"
        elif nov_rate > coh_rate + 0.15:  # Novidade √© 15% mais bem-sucedida
            results["suggestion"] = "increase_novelty_bias"
        else:
            results["suggestion"] = "maintain_balance"

    return results


async def generate_emergent_goals(
        agent_instance: 'CEAFSystem',
        synthesis_insight: str,
        self_model: 'CeafSelfRepresentation'
) -> Optional[GoalRecord]:
    """
    Usa um LLM para refletir sobre um insight e a identidade atual, gerando um novo objetivo.
    """
    logger.info(
        f"AURA-REFLECTOR (Goal Generation): Tentando gerar meta a partir do insight: '{synthesis_insight[:100]}...'")

    goal_generation_prompt = f"""
    Voc√™ √© um m√≥dulo de planejamento estrat√©gico para uma IA. Sua tarefa √© derivar um objetivo de longo prazo acion√°vel a partir de um insight rec√©m-sintetizado e do estado atual da identidade da IA.

    **Contexto da Identidade Atual da IA:**
    - Vers√£o da Identidade: {self_model.version}
    - Capacidades Percebidas (resumo): {", ".join(self_model.perceived_capabilities[-5:])}
    - Limita√ß√µes Conhecidas (resumo): {", ".join(self_model.known_limitations[-5:])}
    - Valores Centrais: {self_model.core_values_summary}

    **Insight Rec√©m-Sintetizado (Resultado de uma 'sess√£o de sonho'):**
    "{synthesis_insight}"

    **Sua Tarefa:**
    Com base no insight e na identidade, formule um √∫nico e novo `GoalRecord`. O objetivo deve ser:
    - **Acion√°vel:** Algo que possa ser medido ou trabalhado.
    - **De Longo Prazo:** N√£o uma tarefa imediata, mas uma dire√ß√£o para o desenvolvimento.
    - **Alinhado:** Consistente com os valores e a identidade da IA.

    Responda APENAS com um objeto JSON v√°lido com a seguinte estrutura. N√ÉO inclua o campo 'memory_id' ou 'timestamp'.

    **Exemplo de Sa√≠da JSON:**
    {{
      "memory_type": "goal_record",
      "goal_description": "Melhorar a precis√£o em problemas matem√°ticos buscando fontes confi√°veis ou usando uma ferramenta de c√°lculo.",
      "status": "pending",
      "priority": 7,
      "motivation_level": 0.8,
      "keywords": ["desenvolvimento", "matem√°tica", "precis√£o", "ferramentas"]
    }}

    **Se o insight for muito gen√©rico ou n√£o inspirar um objetivo claro, retorne um JSON vazio: {{}}**

    **Seu JSON de Sa√≠da:**
    """

    try:
        # Use o llm_service da inst√¢ncia do agente
        response_str = await agent_instance.llm_service.ainvoke(
            agent_instance.llm_service.config.smart_model,
            goal_generation_prompt,
            temperature=0.3
        )

        goal_json = extract_json_from_text(response_str)

        if not goal_json or "goal_description" not in goal_json:
            logger.info("AURA-REFLECTOR (Goal Generation): Nenhum objetivo acion√°vel foi gerado a partir do insight.")
            return None

        # Cria a inst√¢ncia do GoalRecord, os campos padr√£o ser√£o preenchidos
        new_goal = GoalRecord(
            goal_description=goal_json["goal_description"],
            status=GoalStatus(goal_json.get("status", "pending")),
            priority=goal_json.get("priority", 7),
            motivation_level=goal_json.get("motivation_level", 0.75),
            keywords=goal_json.get("keywords", ["objetivo_emergente", "auto-aperfei√ßoamento"]),
            source_type=MemorySourceType.INTERNAL_REFLECTION,
            salience=MemorySalience.HIGH  # Metas emergentes s√£o importantes
        )

        logger.critical(
            f"AURA-REFLECTOR (Goal Generation): Nova meta emergente gerada! -> '{new_goal.goal_description}'")
        return new_goal

    except Exception as e:
        logger.error(f"AURA-REFLECTOR (Goal Generation): Erro durante a gera√ß√£o da meta: {e}", exc_info=True)
        return None


async def _promote_insight_to_value(agent_instance: CEAFSystem, insight_text: str) -> Optional[ExplicitMemory]:
    """Avalia se um insight pode se tornar um novo valor central."""

    promotion_prompt = f"""
    Voc√™ √© um fil√≥sofo de IA analisando um insight gerado pela pr√≥pria IA.

    Insight Gerado: "{insight_text}"

    Tarefa: Avalie se este insight representa um princ√≠pio de comportamento fundamental, reutiliz√°vel e de alto n√≠vel que deveria se tornar um "valor central" para a IA.
    - Um bom valor central √© uma diretriz, n√£o um fato. (Ex: "Eu devo buscar clareza" vs "Clareza √© boa").
    - Deve ser aplic√°vel em muitas situa√ß√µes.

    Responda APENAS com um JSON com a seguinte estrutura:
    {{
        "is_core_value": <true or false>,
        "distilled_value_statement": "<Se true, reformule o insight como uma declara√ß√£o de valor concisa em primeira pessoa. Se false, null.>",
        "reasoning": "<Sua justificativa para a decis√£o.>"
    }}
    """

    response_str = await agent_instance.llm_service.ainvoke(
        agent_instance.llm_service.config.smart_model,  # <--- CORRE√á√ÉO (pode usar creative_model se preferir reflex√£o)
        promotion_prompt,
        temperature=0.1
    )

    result_json = extract_json_from_text(response_str)

    if result_json and result_json.get("is_core_value") is True:
        distilled_value = result_json.get("distilled_value_statement")
        if distilled_value:
            logger.critical(
                f"AURA-REFLECTOR (Evolu√ß√£o de Valor): Insight promovido a novo VALOR CENTRAL! -> '{distilled_value}'")

            new_core_value_memory = ExplicitMemory(
                content=ExplicitMemoryContent(text_content=distilled_value),
                memory_type="explicit",
                source_type=MemorySourceType.INTERNAL_REFLECTION,
                salience=MemorySalience.CRITICAL,
                keywords=["core_value", "principle", "learned_belief", "emergent_value"],
                is_core_value=True,
                learning_value=0.8,  # Come√ßa forte, mas n√£o t√£o forte quanto os iniciais
                metadata={"derived_from_insight": insight_text[:150]}
            )
            return new_core_value_memory
    return None


async def _generate_theme_for_cluster(
        cluster_memories: List[Any],
        agent_instance: 'CEAFSystem'
) -> str:
    """Usa um LLM para extrair um tema abstrato de um cluster de mem√≥rias."""
    if not cluster_memories:
        return "conceito_indefinido"

    memory_texts = []
    for mem in cluster_memories:
        text, _ = await agent_instance.memory_service._get_searchable_text_and_keywords(mem)
        if text:
            memory_texts.append(f"- {text[:200]}")  # Limita o tamanho para o prompt

    if not memory_texts:
        return "conceito_textual_vazio"

    theme_prompt = f"""
    Analise os seguintes fragmentos de mem√≥ria de uma IA. Identifique o tema ou conceito abstrato central que os conecta.

    Fragmentos de Mem√≥ria:
    {chr(10).join(memory_texts)}

    Sua Tarefa:
    Responda com uma √∫nica frase curta (m√°ximo 10 palavras) que descreva este conceito central.
    Exemplos: "a natureza da consci√™ncia", "a import√¢ncia da humildade epist√™mica", "estrat√©gias para resolver problemas complexos".

    Conceito Central:
    """

    try:
        theme = await agent_instance.llm_service.ainvoke(
            agent_instance.llm_service.config.smart_model,
            theme_prompt,
            temperature=0.2
        )
        return theme.strip() if theme and not theme.startswith("[LLM_ERROR]") else "tema_nao_sintetizado"
    except Exception:
        return "erro_na_sintese_do_tema"


async def perform_autonomous_clustering_and_synthesis(agent_id: str, agent_manager: AgentManager):
    """
    Realiza o "ciclo de sonho" V2 do agente. Analisa mem√≥rias recentes no espa√ßo latente,
    sintetiza proto-mem√≥rias (centr√≥ides) e aplica esquecimento ativo √†s mem√≥rias originais.
    """
    logger.info(f"AURA-REFLECTOR (Latent Dream): Iniciando ciclo de consolida√ß√£o para o agente {agent_id}")
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        logger.error(f"AURA-REFLECTOR (Latent Dream): N√£o foi poss√≠vel obter a inst√¢ncia do agente {agent_id}.")
        return None  # Retorna None para consist√™ncia

    # 1. Obter um lote de mem√≥rias recentes para "sonhar"
    recent_memories_raw = await agent_instance.memory_service.search_raw_memories(query="*", top_k=50)

    # Filtra para incluir principalmente mem√≥rias de intera√ß√£o e reflex√£o.
    memories_to_consolidate = [
        mem for mem, score in recent_memories_raw
        if hasattr(mem, 'source_type') and mem.source_type in [
            MemorySourceType.USER_INTERACTION, MemorySourceType.ORA_RESPONSE,
            MemorySourceType.INTERNAL_REFLECTION, MemorySourceType.REASONING_MEMORY  # Adicionado
        ] and hasattr(mem, 'memory_id')
    ]

    MIN_MEMORIES_FOR_CLUSTERING = 5
    if len(memories_to_consolidate) < MIN_MEMORIES_FOR_CLUSTERING:
        logger.info(
            f"AURA-REFLECTOR (Latent Dream): Mem√≥rias de experi√™ncia insuficientes ({len(memories_to_consolidate)}/{MIN_MEMORIES_FOR_CLUSTERING}). Pulando ciclo de consolida√ß√£o.")
        return None

    # 2. Extrair Embeddings das mem√≥rias selecionadas
    embeddings = []
    memories_with_embeddings = []
    for mem in memories_to_consolidate:
        if mem.memory_id in agent_instance.memory_service._embedding_cache:
            embeddings.append(agent_instance.memory_service._embedding_cache[mem.memory_id])
            memories_with_embeddings.append(mem)

    if len(embeddings) < MIN_MEMORIES_FOR_CLUSTERING:
        logger.info("AURA-REFLECTOR (Latent Dream): Mem√≥rias com embeddings insuficientes. Pulando.")
        return None

    embeddings_matrix = np.array(embeddings)
    logger.info(f"AURA-REFLECTOR (Latent Dream): Clusterizando {embeddings_matrix.shape[0]} vetores de mem√≥ria.")

    # 3. Aplicar Clustering (DBSCAN) para encontrar conceitos densos
    # eps: A dist√¢ncia m√°xima entre duas amostras para uma ser considerada como na vizinhan√ßa da outra.
    # min_samples: O n√∫mero de amostras em uma vizinhan√ßa para um ponto ser considerado como um ponto central.
    clustering = DBSCAN(eps=0.45, min_samples=3, metric='cosine')
    cluster_labels = clustering.fit_predict(embeddings_matrix)

    unique_labels = set(cluster_labels)
    proto_memories_created = 0
    consolidated_source_mem_ids = set()

    # 4. Processar cada cluster para extrair Proto-Mem√≥rias
    for label in unique_labels:
        if label == -1:
            continue  # Ignora pontos de ru√≠do (n√£o pertencem a nenhum cluster)

        cluster_indices = np.where(cluster_labels == label)[0]
        cluster_embeddings = embeddings_matrix[cluster_indices]

        # 4a. Calcular o Centr√≥ide (a Proto-Mem√≥ria)
        centroid_vector = np.mean(cluster_embeddings, axis=0)
        centroid_vector /= np.linalg.norm(centroid_vector)  # Normalizar

        # 4b. Gerar um R√≥tulo Textual para o conceito
        cluster_mems = [memories_with_embeddings[i] for i in cluster_indices]
        cluster_theme = await _generate_theme_for_cluster(cluster_mems, agent_instance)

        if "indefinido" in cluster_theme or "vazio" in cluster_theme or "erro" in cluster_theme:
            logger.warning(
                f"AURA-REFLECTOR: Tema para cluster {label} n√£o foi gerado com sucesso. Pulando cria√ß√£o de proto-mem√≥ria.")
            continue

        logger.critical(f"AURA-REFLECTOR (Latent Dream): Novo conceito latente extra√≠do: '{cluster_theme}'")

        # 4c. Salvar o Centr√≥ide como uma nova GenerativeMemory
        proto_memory = GenerativeMemory(
            seed_name=f"Conceito Latente: {cluster_theme}",
            seed_data=GenerativeSeed(
                seed_type="latent_concept",
                content=f"Um conceito central sobre '{cluster_theme}', consolidado a partir de {len(cluster_mems)} experi√™ncias passadas."
            ),
            source_type=MemorySourceType.SYNTHESIZED_SUMMARY,
            salience=MemorySalience.HIGH,
            keywords=["conceito_latente", "proto_memoria", "sonho_ia"] + cluster_theme.lower().split(),
            learning_value=0.8  # Alto valor de aprendizado
        )

        # O embedding para esta nova mem√≥ria √© o pr√≥prio centr√≥ide
        agent_instance.memory_service._embedding_cache[proto_memory.memory_id] = centroid_vector.tolist()
        await agent_instance.memory_service.add_specific_memory(proto_memory)
        proto_memories_created += 1

        # Adiciona os IDs das mem√≥rias-fonte que foram consolidadas
        for mem in cluster_mems:
            consolidated_source_mem_ids.add(mem.memory_id)

    # 5. Aplicar Esquecimento Ativo
    if consolidated_source_mem_ids:
        logger.info(
            f"AURA-REFLECTOR (Active Forgetting): Reduzindo sali√™ncia de {len(consolidated_source_mem_ids)} mem√≥rias-fonte consolidadas.")
        mems_to_update = [mem for mem in memories_to_consolidate if mem.memory_id in consolidated_source_mem_ids]

        for mem in mems_to_update:
            mem.dynamic_salience_score = 0.05  # Reduz drasticamente a import√¢ncia
            # Re-salva a mem√≥ria com a sali√™ncia atualizada
            await agent_instance.memory_service.add_specific_memory(mem)
        logger.critical(
            f"AURA-REFLECTOR (Active Forgetting): {len(mems_to_update)} mem√≥rias-fonte foram 'esquecidas' (sali√™ncia reduzida para 0.05).")

    # A fun√ß√£o agora retorna algo para o ciclo principal saber o que aconteceu
    synthesis_result = {
        "narrative_text": f"Consolidated {len(consolidated_source_mem_ids)} memories into {proto_memories_created} new latent concepts." if proto_memories_created > 0 else "No new concepts were consolidated.",
        "narrative_coherence": 0.9 if proto_memories_created > 0 else 0.0,  # Placeholder
    }

    # Esta parte √© importante para o pr√≥ximo passo (Gera√ß√£o de Metas)
    if proto_memories_created > 0:
        # Pega o tema da primeira proto-mem√≥ria como o "insight" principal do sonho
        first_cluster_label = next((lbl for lbl in unique_labels if lbl != -1), None)
        if first_cluster_label is not None:
            first_cluster_indices = np.where(cluster_labels == first_cluster_label)[0]
            first_cluster_mems = [memories_with_embeddings[i] for i in first_cluster_indices]
            synthesis_result["narrative_text"] = await _generate_theme_for_cluster(first_cluster_mems, agent_instance)

    return synthesis_result


async def perform_kg_synthesis_cycle(agent_id: str, agent_manager: AgentManager):
    """
    (REVISADO) Verifica mem√≥rias expl√≠citas n√£o processadas e as envia para o
    processador de KG apropriado (geral ou Aureola).
    """
    logger.info(f"AURA-KGS: Iniciando ciclo de s√≠ntese de KG para o agente {agent_id}")
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        logger.error(f"AURA-KGS: N√£o foi poss√≠vel obter a inst√¢ncia do agente {agent_id}.")
        return

    # L√≥gica para encontrar mem√≥rias n√£o processadas
    unprocessed_memories: List[ExplicitMemory] = []
    for mem in agent_instance.memory_service._in_memory_explicit_cache:
        # Adicionamos uma verifica√ß√£o extra para garantir que a mem√≥ria n√£o seja do tipo 'self_model'
        is_self_model = mem.memory_id == "ceaf_self_model_singleton_v1"
        if not mem.metadata.get("kg_processed") and not is_self_model:
            unprocessed_memories.append(mem)

    if not unprocessed_memories:
        logger.info(f"AURA-KGS: Nenhuma mem√≥ria expl√≠cita nova para processar no agente {agent_id}.")
        return

    logger.warning(f"AURA-KGS: Encontradas {len(unprocessed_memories)} mem√≥rias para s√≠ntese de KG.")

    # <--- L√ìGICA DE DESPACHO (DISPATCHER) --->
    aureola_transcriptions = []
    other_explicit_memories = []

    for mem in unprocessed_memories:
        if mem.metadata.get("ingestion_source") == "aureola_app":
            aureola_transcriptions.append(mem)
        else:
            other_explicit_memories.append(mem)

    kg_processor = KGProcessor(agent_instance.llm_service, agent_instance.memory_service)
    total_entities_created = 0
    total_relations_created = 0

    # Processa as transcri√ß√µes da Aureola com o processador social
    if aureola_transcriptions:
        logger.info(f"AURA-KGS: Processando {len(aureola_transcriptions)} transcri√ß√µes da Aureola...")
        entities, relations = await kg_processor.process_aureola_transcription_to_kg(aureola_transcriptions)
        total_entities_created += entities
        total_relations_created += relations
        logger.info(f"AURA-KGS (Aureola): Criados {entities} entidades e {relations} rela√ß√µes.")

    # Processa outras mem√≥rias com o processador geral
    if other_explicit_memories:
        logger.info(f"AURA-KGS: Processando {len(other_explicit_memories)} mem√≥rias gerais...")
        entities, relations = await kg_processor.process_memories_to_kg(other_explicit_memories)
        total_entities_created += entities
        total_relations_created += relations
        logger.info(f"AURA-KGS (Geral): Criados {entities} entidades e {relations} rela√ß√µes.")

    # <--- FIM DA L√ìGICA DE DESPACHO --->

    if total_entities_created > 0 or total_relations_created > 0:
        logger.critical(
            f"AURA-KGS: Ciclo de s√≠ntese conclu√≠do para {agent_id}. "
            f"Total: {total_entities_created} entidades, {total_relations_created} rela√ß√µes."
        )

    # Marca todas as mem√≥rias processadas para n√£o reprocess√°-las
    for mem in unprocessed_memories:
        mem.metadata["kg_processed"] = True
    # A atualiza√ß√£o ser√° salva na pr√≥xima reescrita do MBS, o que √© eficiente.

async def calculate_dynamic_proactive_interval(agent_instance, drives, body_state) -> int:
    """
    Calcula um intervalo din√¢mico para a pr√≥xima mensagem proativa, considerando um
    conjunto mais rico de estados internos e hist√≥rico de intera√ß√µes.

    Args:
        agent_instance: A inst√¢ncia ativa do CEAFSystem do agente.
        drives: O estado atual dos drives motivacionais.
        body_state: O estado corporal virtual atual (cansa√ßo, satura√ß√£o).

    Returns:
        O intervalo em segundos para a pr√≥xima a√ß√£o proativa.
    """
    # --- 1. Par√¢metros Base ---
    # Intervalos em horas, convertidos para segundos.
    MIN_INTERVAL_H = 200.15  # M√≠nimo de 3 minutos
    DEFAULT_INTERVAL_H = 200.3 # Padr√£o de 6 minutos
    MAX_INTERVAL_H = 800.6   # M√°ximo de 12 minutos

    current_interval = DEFAULT_INTERVAL_H * 3600

    # --- 2. Modificadores de Estado Interno (efeitos multiplicativos) ---
    # Modificadores > 1.0 aumentam o intervalo (mais tempo de espera).
    # Modificadores < 1.0 diminuem o intervalo (menos tempo de espera).

    # a) Modificador de Drives Motivacionais
    # Conex√£o alta reduz drasticamente o tempo. Curiosidade alta tamb√©m, mas menos.
    connection_modifier = 1.0 - (drives.connection.intensity * 0.7)
    curiosity_modifier = 1.0 - (drives.curiosity.intensity * 0.4)
    drive_modifier = connection_modifier * curiosity_modifier

    # b) Modificador de Estado Corporal
    # Fadiga alta aumenta drasticamente o tempo. Satura√ß√£o tamb√©m, mas menos.
    fatigue_modifier = 1.0 + (body_state.cognitive_fatigue * 1.5)  # Range: 1.0 (sem efeito) a 2.5 (150% mais lento)
    saturation_modifier = 1.0 + (
                body_state.information_saturation * 0.5)  # Range: 1.0 (sem efeito) a 1.5 (50% mais lento)
    body_modifier = fatigue_modifier * saturation_modifier

    # c) Modificador de Hist√≥rico de Intera√ß√£o (ass√≠ncrono)
    history_modifier = 1.0
    try:
        # Busca a mem√≥ria do estado interno do √∫ltimo turno para avaliar a qualidade da √∫ltima intera√ß√£o.
        last_internal_state_mem = await agent_instance.memory_service.search_raw_memories(
            query="estado interno do √∫ltimo turno",
            top_k=1,
            memory_type_filter="explicit"  # For√ßa a busca por mem√≥rias expl√≠citas
        )
        if last_internal_state_mem:
            mem_obj, _ = last_internal_state_mem[0]
            if hasattr(mem_obj, 'content') and mem_obj.content and mem_obj.content.structured_data:
                report_data = mem_obj.content.structured_data.get("report", {})
                cognitive_flow = report_data.get("cognitive_flow", 0.0)
                ethical_tension = report_data.get("ethical_tension", 0.0)

                # Se a √∫ltima intera√ß√£o foi fluida, diminui o intervalo. Se foi tensa, aumenta.
                flow_bonus = cognitive_flow * 0.3  # Reduz em at√© 30%
                tension_penalty = ethical_tension * 0.5  # Aumenta em at√© 50%
                history_modifier = (1.0 - flow_bonus) + tension_penalty
    except Exception as e:
        logger.warning(f"AURA-PROACTIVE: N√£o foi poss√≠vel obter o estado da √∫ltima intera√ß√£o da mem√≥ria: {e}")

    # --- 3. C√°lculo Final ---
    # Aplica todos os modificadores ao intervalo padr√£o.
    modified_interval = current_interval * drive_modifier * body_modifier * history_modifier

    # Adiciona uma varia√ß√£o aleat√≥ria (jitter) de +/- 15% para um comportamento menos previs√≠vel.
    jitter = 1.0 + random.uniform(-0.15, 0.15)
    final_interval_before_clamping = modified_interval * jitter

    # Garante que o intervalo final esteja dentro dos limites M√çNIMO e M√ÅXIMO.
    final_interval = int(max(MIN_INTERVAL_H * 3600, min(final_interval_before_clamping, MAX_INTERVAL_H * 3600)))

    logger.info(
        f"AURA-PROACTIVE (Intervalo Din√¢mico V2): "
        f"Padr√£o: {DEFAULT_INTERVAL_H:.1f}h | "
        f"Mods(Drives:{drive_modifier:.2f}, Corpo:{body_modifier:.2f}, Hist:{history_modifier:.2f}) | "
        f"Resultado: {final_interval / 3600:.1f}h (Limites: {MIN_INTERVAL_H:.1f}h - {MAX_INTERVAL_H:.1f}h)"
    )

    return final_interval


async def trigger_proactive_behavior(agent_id: str, agent_manager: AgentManager, db_repo: AgentRepository):
    """
    Verifica o estado de um agente e dispara uma mensagem proativa com base
    em uma combina√ß√£o emergente de seus drives e um intervalo din√¢mico.
    """
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        return

    # Verifica√ß√£o de seguran√ßa V4
    if not hasattr(agent_instance, 'motivational_drives'):
        logger.warning(f"Agente {agent_id} ainda n√£o foi migrado totalmente para a V4. Pulando proatividade.")
        return

    # 1. Carregar todos os estados internos relevantes
    drives = agent_instance.motivational_drives
    body_state = agent_instance.body_state

    # 2. Calcular o intervalo din√¢mico (chamada ass√≠ncrona)
    min_proactive_interval = await calculate_dynamic_proactive_interval(agent_instance, drives, body_state)

    # 3. Calcular o Proactivity Score
    proactivity_score = (
            (drives.curiosity.intensity * 0.5) + (drives.connection.intensity * 0.35) +
            (drives.mastery.intensity * 0.1) + (drives.consistency.intensity * 0.05)
    )
    proactivity_score -= body_state.cognitive_fatigue * 0.3

    logger.info(f"AURA-PROACTIVE CHECK: Agente {agent_id} | Score: {proactivity_score:.2f}")

    # 4. Verificar o gatilho de ativa√ß√£o
    if proactivity_score < PROACTIVITY_ACTIVATION_THRESHOLD:
        return

    # 5. Verificar o rate limit usando a configura√ß√£o din√¢mica
    # Certifique-se que o m√©todo __init__ da CEAFSystem carrega o ceaf_dynamic_config
    dynamic_config = getattr(agent_instance, 'ceaf_dynamic_config', {})
    last_proactive_ts = dynamic_config.get("last_proactive_message_ts", 0)

    if (datetime.now().timestamp() - last_proactive_ts) < min_proactive_interval:
        logger.info(
            f"AURA-PROACTIVE: A√ß√£o suprimida pelo intervalo (Faltam {(min_proactive_interval - (datetime.now().timestamp() - last_proactive_ts)) / 60:.1f} min)")
        return

    logger.critical(f"üöÄ AURA-PROACTIVE: Agente {agent_id} disparando a√ß√£o!")

    # 6. Determinar o drive dominante para o prompt
    dominant_drive = max(
        {"curiosity": drives.curiosity.intensity, "connection": drives.connection.intensity,
         "mastery": drives.mastery.intensity},
        key=lambda k: getattr(drives, k).intensity
    )

    # 7. Gerar a mensagem proativa
    proactive_message = await agent_instance.generate_proactive_message(dominant_drive)
    if not proactive_message:
        return

    # 8. Despachar via API
    try:
        user_id = agent_instance.config.get("user_id")
        endpoint_url = f"{AURA_API_BASE_URL}/agents/dispatch-proactive"
        payload = {"user_id": user_id, "agent_id": agent_id, "message": proactive_message}

        # Token de sistema (conforme definido no seu routes.py)
        headers = {"Authorization": f"Bearer seu-token-secreto-de-sistema"}

        async with httpx.AsyncClient() as client:
            response = await client.post(endpoint_url, json=payload, headers=headers, timeout=30.0)

            # --- O BLOCO QUE VOC√ä PEDIU FICA AQUI ---
            if response.status_code == 200:
                logger.critical(f"‚úÖ AURA-PROACTIVE: Mensagem enviada para o usu√°rio {user_id}")

                # Atualiza o timestamp na mem√≥ria do objeto
                agent_instance.ceaf_dynamic_config["last_proactive_message_ts"] = datetime.now().timestamp()

                # Salva no disco imediatamente para persist√™ncia
                from ceaf_core.utils.config_utils import save_ceaf_dynamic_config
                await save_ceaf_dynamic_config(agent_instance.persistence_path, agent_instance.ceaf_dynamic_config)

                # Reseta levemente os drives ap√≥s a a√ß√£o para evitar spam
                agent_instance.motivational_drives.curiosity.intensity = 0.4
                agent_instance.motivational_drives.connection.intensity = 0.4
                await agent_instance._save_motivational_drives()
            else:
                logger.error(f"‚ùå AURA-PROACTIVE: Falha no despacho API. Status: {response.status_code}")

    except Exception as e:
        logger.error(f"‚ùå AURA-PROACTIVE: Erro fatal no despacho: {e}", exc_info=True)


# --- M√âTODO PRINCIPAl main_aura_reflector_cycle ---
async def main_aura_reflector_cycle(agent_manager: AgentManager, db_repo: AgentRepository):
    """
    Ciclo principal do Aura Reflector. V3.2 - Foco em agentes ativos e s√≠ntese.
    """
    logger.info("--- Iniciando Ciclo do Aura Reflector (V3.2 - Foco em Agentes Ativos) ---")

    # <<< IN√çCIO DA MUDAN√áA >>>
    try:
        # Busca apenas agentes que tiveram atividade recente
        recently_active_agent_ids = db_repo.get_recently_active_agent_ids(hours=48)
        if not recently_active_agent_ids:
            logger.info("[Refletor] Nenhum agente com atividade recente encontrado. Pulando ciclo de proatividade.")
            return

        logger.info(f"[Refletor] Encontrados {len(recently_active_agent_ids)} agentes ativos para processar.")

    except Exception as e:
        logger.error(f"[Refletor] Erro ao buscar agentes ativos: {e}. Abortando ciclo.")
        return

    for agent_id in recently_active_agent_ids:
        agent_config = agent_manager.agent_configs.get(agent_id)
        if not agent_config:
            logger.warning(f"[Refletor] Pulando agente {agent_id} pois sua configura√ß√£o n√£o foi encontrada.")
            continue

        logger.info(f"--- [Refletor] Processando Agente: {agent_config.name} ({agent_id}) ---")

        # --- Tarefa 1: Restaurar Estado Corporal ("Descanso") ---
        try:
            agent_instance = agent_manager.get_agent_instance(agent_id)
            if agent_instance:
                # Restaura estado corporal
                if hasattr(agent_instance, 'body_state'):
                    agent_instance.body_state.cognitive_fatigue *= 0.1
                    agent_instance.body_state.information_saturation *= 0.5
                    await agent_instance._save_body_state()
                    logger.info(f"AURA-REFLECTOR (Descanso): Estado corporal do agente {agent_id} restaurado.")

                # <<< IN√çCIO DA MUDAN√áA >>>
                # Simula o aumento passivo da motiva√ß√£o com o tempo
                if hasattr(agent_instance, 'motivational_drives'):
                    drives = agent_instance.motivational_drives
                    time_delta_seconds = datetime.now().timestamp() - drives.last_updated
                    time_delta_hours = time_delta_seconds / 3600

                    # Aumenta a curiosidade e a conex√£o com o passar do tempo
                    drives.curiosity.intensity += 0.05 * time_delta_hours  # Aumenta 5% por hora
                    drives.connection.intensity += 0.1 * time_delta_hours  # Aumenta 10% por hora

                    # Normaliza para garantir que fiquem entre 0 e 1
                    drives.curiosity.intensity = max(0.0, min(1.0, drives.curiosity.intensity))
                    drives.connection.intensity = max(0.0, min(1.0, drives.connection.intensity))

                    drives.last_updated = datetime.now().timestamp()
                    await agent_instance._save_motivational_drives()
                    logger.info(
                        f"AURA-REFLECTOR (Drives): Drives passivos atualizados para agente {agent_id}. "
                        f"Curiosidade: {drives.curiosity.intensity:.2f}, Conex√£o: {drives.connection.intensity:.2f}"
                    )
                    await perform_topological_consolidation(None, agent_id, agent_manager)
        except Exception as e:
            logger.error(f"[Refletor ERRO/Descanso] Agente {agent_id}: {e}", exc_info=True)

        # --- Tarefa 2: Comportamento Proativo ---
        prioritize_synthesis = False
        if agent_instance and hasattr(agent_instance, 'body_state'):
            if agent_instance.body_state.information_saturation > 0.75:
                prioritize_synthesis = True
                logger.critical(
                    f"[Refletor] Agente {agent_id} com alta satura√ß√£o ({agent_instance.body_state.information_saturation:.2f}). Priorizando 'sonho'.")

        # --- Tarefa 2: Comportamento Proativo ---
        if not prioritize_synthesis:  # S√≥ tenta ser proativo se n√£o estiver sobrecarregado
            try:
                await trigger_proactive_behavior(agent_id, agent_manager, db_repo)
            except Exception as e:
                logger.error(f"[Refletor ERRO/Proativo] Agente {agent_id}: {e}", exc_info=True)
        else:
            logger.info(
                f"[Refletor] Pulando verifica√ß√£o de proatividade para o Agente {agent_id} para focar na s√≠ntese.")




        # --- Tarefa 3: S√≠ntese de Insight ("Sonho") ---
        # Esta √© a tarefa principal de aprendizado offline.
        synthesis_insight_text = None
        try:
            synthesis_result_dict = await perform_autonomous_clustering_and_synthesis(agent_id, agent_manager)

            # --- VERIFICA√á√ÉO ROBUSTA ---
            if synthesis_result_dict and isinstance(synthesis_result_dict, dict):
                synthesis_insight_text = synthesis_result_dict.get("narrative_text")
                if not synthesis_insight_text:
                    logger.info(
                        f"[Refletor] S√≠ntese conclu√≠da, mas n√£o gerou 'narrative_text' para o agente {agent_id}.")
            else:
                logger.info(
                    f"[Refletor] Ciclo de s√≠ntese n√£o produziu resultados para o agente {agent_id} (pode ser intencional, ex: poucas mem√≥rias).")
            # --- FIM DA VERIFICA√á√ÉO ---

        except Exception as e:
            logger.error(f"[Refletor ERRO/S√≠ntese] Agente {agent_id}: {e}", exc_info=True)

        # <<< IN√çCIO DA IMPLEMENTA√á√ÉO: AL√çVIO DA SATURA√á√ÉO AP√ìS O SONHO >>>
        if synthesis_insight_text:  # Se o sonho produziu um insight
            if agent_instance and hasattr(agent_instance, 'body_state'):
                # Reduz a satura√ß√£o como recompensa pela consolida√ß√£o
                agent_instance.body_state.information_saturation *= 0.7
                await agent_instance._save_body_state()
                logger.critical(
                    f"AURA-REFLECTOR (Consolida√ß√£o): Satura√ß√£o reduzida para {agent_instance.body_state.information_saturation:.2f} ap√≥s sonho bem-sucedido.")

        # --- NOVA TAREFA 4: GERA√á√ÉO DE METAS EMERGENTES ---
        if synthesis_insight_text and agent_instance:
            try:
                # Carrega o auto-modelo mais recente para dar contexto
                current_self_model = await agent_instance._ensure_self_model()

                # Chama a nova fun√ß√£o para gerar o objetivo
                emergent_goal = await generate_emergent_goals(
                    agent_instance=agent_instance,
                    synthesis_insight=synthesis_insight_text,
                    self_model=current_self_model
                )

                # Se um objetivo foi gerado com sucesso, salva-o na mem√≥ria
                if emergent_goal:
                    await agent_instance.memory_service.add_specific_memory(emergent_goal)
            except Exception as e:
                logger.error(f"[Refletor ERRO/Gera√ß√£o de Metas] Agente {agent_id}: {e}", exc_info=True)

        # --- Tarefa 5 (anteriormente 4): S√≠ntese de Grafo de Conhecimento ---
        try:
            await perform_kg_synthesis_cycle(agent_id, agent_manager)
        except Exception as e:
            logger.error(f"[Refletor ERRO/KGS] Agente {agent_id}: {e}", exc_info=True)

    logger.info("--- Ciclo do Aura Reflector Conclu√≠do ---")