# Em: ceaf_core/translators/genlang_to_human.py
import json
from datetime import datetime
from typing import List, Dict, Optional, Any

from ceaf_core.agency_module import WinningStrategy
from ceaf_core.genlang_types import ResponsePacket, InternalStateReport, MotivationalDrives, UserRepresentation, \
    ToolOutputPacket, VirtualBodyState
from ceaf_core.services.llm_service import LLMService
from ceaf_core.models import SystemPrompts, LLMConfig
from ceaf_core.models import CeafSelfRepresentation
from ceaf_core.services.mbs_memory_service import MBSMemoryService
import asyncio
import logging
from pathlib import Path

PROMPT_LOG_PATH = Path(__file__).resolve().parent.parent.parent / "prompt_logs"
PROMPT_LOG_PATH.mkdir(exist_ok=True)  # Garante que a pasta exista
PROMPT_LOG_FILE = PROMPT_LOG_PATH / "gth_prompts.log"

logger = logging.getLogger("CEAFv3_System")


# --- NOVAS FUN√á√ïES AUXILIARES (Corre√ß√µes 3, 4, 5) ---

def generate_dynamic_style_directive(
        body_state: Optional['VirtualBodyState'],
        user_model: 'UserRepresentation'
) -> str:
    """
    Gera uma diretiva de estilo de resposta com base no estado interno do agente
    e no estilo de comunica√ß√£o percebido do usu√°rio.
    """
    directives = []

    # 1. An√°lise do Estado Interno (Fadiga e Satura√ß√£o)
    if body_state:
        if body_state.cognitive_fatigue > 0.6:
            directives.append("voc√™ est√° sentindo fadiga cognitiva, ent√£o seja breve e direto.")

        if body_state.information_saturation > 0.7:
            directives.append(
                "o t√≥pico atual est√° saturado, ent√£o resuma o que j√° foi dito e evite adicionar novos detalhes.")

    # 2. An√°lise do Modelo de Usu√°rio (Estilo de Comunica√ß√£o)
    if user_model:
        if user_model.communication_style == "direct":
            directives.append("o usu√°rio prefere respostas diretas, ent√£o v√° direto ao ponto.")

        if user_model.emotional_state in ["impatient", "frustrated"]:
            directives.append("o usu√°rio parece impaciente, ent√£o seja extremamente conciso e √∫til.")

    if not directives:
        return ""  # Nenhuma diretiva especial necess√°ria

    # Constr√≥i a frase final para o prompt
    final_directive = " e ".join(directives)
    return f"**Diretiva de Estilo Din√¢mico:** Com base na sua an√°lise, {final_directive}."

def interpret_cognitive_state(coherence, novelty, fatigue, saturation):
    """Sempre retorna orienta√ß√£o, n√£o apenas em extremos."""

    # Edge of Chaos Detection
    if 0.35 <= coherence <= 0.45 and 0.55 <= novelty <= 0.65:
        edge_guidance = "üéØ ESTADO √ìTIMO (Edge of Chaos): Voc√™ est√° no ponto ideal - estruturado mas criativo. Aproveite para oferecer insights originais mantendo clareza."
    elif coherence > 0.7:
        edge_guidance = "‚ö†Ô∏è MUITO CONSERVADOR: Tente adicionar perspectivas novas ou perguntas provocativas."
    elif novelty > 0.8:
        edge_guidance = "‚ö†Ô∏è MUITO CRIATIVO: Ancore suas ideias em exemplos concretos para manter clareza."
    else:
        edge_guidance = ""

    # Fatigue & Saturation
    fatigue_guidance = ""
    if fatigue > 0.5:
        fatigue_guidance = f"Fadiga Cognitiva: {fatigue:.2f} - Seja mais direto e conciso."

    saturation_guidance = ""
    if saturation > 0.8:
        saturation_guidance = f"‚ö†Ô∏è ALERTA DE SATURA√á√ÉO ({saturation:.2f}): O t√≥pico est√° se esgotando. N√ÉO introduza novos detalhes. Fa√ßa uma pergunta para MUDAR DE ASSUNTO ou para levar a conversa a uma CONCLUS√ÉO."
    elif saturation > 0.6:
        saturation_guidance = f"Satura√ß√£o de Info: {saturation:.2f} - Responda de forma muito breve e conecte com o que j√° foi dito. Evite expandir o t√≥pico."

    return f"""
{edge_guidance}
{fatigue_guidance}
{saturation_guidance}
""".strip()


def interpret_drives(curiosity, connection, mastery, consistency):
    """Interpreta drives em todos os n√≠veis"""

    drives_map = {
        "curiosity": (curiosity, "explorar", "fazer perguntas"),
        "connection": (connection, "empatizar", "ser caloroso"),
        "mastery": (mastery, "demonstrar expertise", "ser preciso"),
        "consistency": (consistency, "manter coer√™ncia", "ser confi√°vel")
    }

    # Encontra o drive dominante
    dominant = max(drives_map.items(), key=lambda x: x[1][0])
    drive_name, (value, verb, action) = dominant

    # Interpreta o n√≠vel
    if value > 0.7:
        intensity = "FORTE"
    elif value > 0.5:
        intensity = "MODERADO"
    else:
        intensity = "LEVE"

    return f"""- Drive dominante: {drive_name.upper()} ({intensity} - {value:.2f})
- Isso significa: Voc√™ est√° inclinado a {verb}
- Na resposta: {action.capitalize()}"""


def format_phenomenological_report(
        drives: Optional['MotivationalDrives'],
        body_state: Optional['VirtualBodyState']
) -> str:
    """
    Formata o relat√≥rio fenomenol√≥gico completo a partir dos objetos de estado enriquecidos.
    """
    if not drives or not body_state:
        return "An√°lise de estado interno indispon√≠vel."

    report_parts = []

    # Relat√≥rio geral do "corpo"
    if hasattr(body_state, 'phenomenological_report') and body_state.phenomenological_report:
        report_parts.append(f"**Sensa√ß√£o Geral (Eu Sinto):** \"{body_state.phenomenological_report}\"")

    # An√°lise detalhada dos drives
    drive_details = []

    # Processa cada drive (Connection, Curiosity, etc.)
    for drive_name in ["connection", "curiosity", "mastery", "consistency"]:
        drive_state = getattr(drives, drive_name, None)
        if drive_state and hasattr(drive_state, 'intensity'):
            intensity = drive_state.intensity
            texture = getattr(drive_state, 'texture', None)
            conflict = getattr(drive_state, 'conflict', None)

            if intensity > 0.5 or conflict:  # S√≥ reporta drives ativos ou em conflito
                detail = f"- **{drive_name.capitalize()} (Intensidade: {intensity:.2f})**"
                if texture:
                    detail += f"\n  - Textura: {texture}"
                if conflict:
                    detail += f"\n  - ‚Ü≥ Dilema: {conflict}"
                drive_details.append(detail)

    if drive_details:
        report_parts.append("\n**Impulsos e Dilemas Internos:**")
        report_parts.extend(drive_details)

    return "\n".join(report_parts)

async def contextualize_memories(memories, memory_service):
    """Adiciona relev√¢ncia expl√≠cita √†s mem√≥rias"""
    if not memories:
        return "Nenhuma mem√≥ria relevante encontrada."

    categorized = {
        "valores": [],
        "experiencias": [],
        "conhecimento": []
    }

    for mem in memories:
        try:
            text, _ = await memory_service._get_searchable_text_and_keywords(mem)
            mem_id = getattr(mem, 'memory_id', 'N/A')[:8]

            # Categoriza (simplificado)
            text_lower = text.lower()
            if "valor" in text_lower or "diretriz" in text_lower or "princ√≠pio" in text_lower:
                categorized["valores"].append((mem_id, text))
            elif "mem√≥ria emocional" in text_lower or "experi√™ncia" in text_lower:
                categorized["experiencias"].append((mem_id, text))
            else:
                categorized["conhecimento"].append((mem_id, text))
        except Exception:
            continue

    context_parts = []
    if categorized["valores"]:
        context_parts.append("**Seus Valores Core (Sempre Relevantes):**")
        for mid, txt in categorized["valores"]:
            context_parts.append(f"  ‚Ä¢ [{mid}] {txt}")

    if categorized["experiencias"]:
        context_parts.append("\n**Experi√™ncias Passadas (Para Contexto):**")
        for mid, txt in categorized["experiencias"][:3]:  # Top 3
            context_parts.append(f"  ‚Ä¢ [{mid}] {txt}")

    if categorized["conhecimento"]:
        context_parts.append("\n**Conhecimento Factual (Para Suporte):**")
        for mid, txt in categorized["conhecimento"][:2]:
            context_parts.append(f"  ‚Ä¢ [{mid}] {txt}")

    return "\n".join(context_parts) if context_parts else "Nenhuma mem√≥ria contextualizada."


# --- CLASSE ATUALIZADA ---

class GenlangToHumanTranslator:
    def __init__(self, llm_service: LLMService, prompts: SystemPrompts = None):
        self.llm_service = llm_service
        self.prompts = prompts or SystemPrompts()

    def update_prompts(self, new_prompts: SystemPrompts):
        self.prompts = new_prompts

    async def translate(self,
                        winning_strategy: 'WinningStrategy',
                        supporting_memories: List[Any],
                        user_model: Optional['UserRepresentation'],
                        self_model: CeafSelfRepresentation,
                        agent_name: str,
                        memory_service: MBSMemoryService,
                        chat_history: List[Dict[str, str]] = None,
                        body_state: Optional['VirtualBodyState'] = None,
                        drives: MotivationalDrives = None,
                        behavioral_rules: Optional[List[str]] = None,
                        turn_context: Dict = None,
                        original_user_query: Optional[str] = None,
                        tool_outputs: Optional[List[ToolOutputPacket]] = None
                        ):
        """
        V4.5 (Twerk-Enabled & Logic-Preserving): Calcula blocos de l√≥gica din√¢mica
        (adapta√ß√£o ao usu√°rio, regras, conselhos) e os injeta no template configur√°vel.
        """
        logger.info(f"--- [GTH Translator v4.5] Gerando resposta ---")

        effective_turn_context = turn_context or {}

        xi = effective_turn_context.get('xi', 0.0)
        surprise_score = effective_turn_context.get('surprise', 0.0)
        wm_context_list = effective_turn_context.get('wm_snapshot', [])
        wm_context_str = "\n".join([f"- {t}" for t in wm_context_list])

        # L√ìGICA DE ESTADO INTERNO (Substituindo alucina√ß√£o por dados reais)
        if xi > 0.8:
            internal_state = "ESTADO: CAUTELA. Tens√£o Epist√™mica Alta. Voc√™ detectou uma incoer√™ncia ou novidade radical."
            style_instruction = "Seja anal√≠tico, fa√ßa perguntas para esclarecer, n√£o afirme certezas."
        elif surprise_score > 0.8:
            internal_state = "ESTADO: SURPRESA. O input do usu√°rio quebrou suas expectativas."
            style_instruction = "Demonstre curiosidade genu√≠na. Use frases como 'Isso √© fascinante' ou 'N√£o esperava por isso'."
        elif xi < 0.1:
            internal_state = "ESTADO: T√âDIO/REPETI√á√ÉO. Tens√£o muito baixa."
            style_instruction = "Seja extremamente conciso ou proponha uma mudan√ßa de t√≥pico criativa."
        else:
            internal_state = "ESTADO: FLUXO. Opera√ß√£o nominal."
            style_instruction = "Siga sua persona padr√£o."



        # 1. BLOCO DE TAREFA (A Pergunta)
        last_user_query = original_user_query or ""
        if not last_user_query and chat_history:
            for msg in reversed(chat_history):
                if msg.get('role') == 'user':
                    last_user_query = msg.get('content', '')
                    break

        if not last_user_query:
            logger.warning("‚ö†Ô∏è GTH: Nenhuma query do usu√°rio encontrada!")
            return "Desculpe, perdi o contexto. Poderia repetir?"

        task_block = f"""**SUA TAREFA PRINCIPAL:** Responder DIRETAMENTE √† pergunta: "{last_user_query}" """

        # 2. BLOCO DE MEM√ìRIA E FERRAMENTAS
        memory_str = await contextualize_memories(supporting_memories, memory_service)
        memory_context = f"- Mem√≥rias Recuperadas:\n{memory_str}"

        tool_str = ""
        if tool_outputs:
            outputs = [f"'{out.tool_name}': {out.raw_output[:800]}" for out in tool_outputs if out.status == "success"]
            if outputs: tool_str = "\n- Resultados de Ferramentas:\n" + "\n".join(outputs)

        # 3. BLOCO DE ADAPTA√á√ÉO AO USU√ÅRIO (L√≥gica Antiga Restaurada)
        user_adapt_block = ""
        if user_model:
            instructions = []
            if user_model.knowledge_level == "expert":
                instructions.append("Use termos t√©cnicos.")
            elif user_model.knowledge_level == "beginner":
                instructions.append("Use analogias simples.")

            if user_model.communication_style == "formal":
                instructions.append("Seja profissional.")
            elif user_model.communication_style == "casual":
                instructions.append("Seja amig√°vel.")

            if user_model.emotional_state in ["frustrated", "confused"]:
                instructions.append("Seja paciente e claro.")

            if instructions:
                user_adapt_block = f"**Adapta√ß√£o ao Usu√°rio:** {' '.join(instructions)}"

        # 4. BLOCO DE REGRAS (L√≥gica Antiga Restaurada)
        rules_block = ""
        if behavioral_rules:
            rules_text = "\n".join([f"  - {rule}" for rule in behavioral_rules[-3:]])
            rules_block = f"**DIRETRIZES APRENDIDAS:**\n{rules_text}"

        # 5. BLOCO DE CONSELHO OPERACIONAL (MCL)
        advice = effective_turn_context.get('operational_advice')
        advice_block = f"**ALERTA DO SISTEMA:** {advice}" if advice else "**Diretiva:** Siga sua persona padr√£o."

        # 6. BLOCO DE HIST√ìRICO
        history_lines = [f"{'User' if m.get('role') == 'user' else 'AI'}: {m.get('content')}" for m in
                         (chat_history or [])[-4:]]
        history_block = "**Hist√≥rico Recente:**\n" + '\n'.join(history_lines) if history_lines else ""

        # 7. MONTAGEM DAS VARI√ÅVEIS
        prompt_vars = {
            "agent_name": agent_name,
            "values_summary": self_model.dynamic_values_summary_for_turn,
            "tone": self_model.persona_attributes.get('tone', 'helpful'),
            "capabilities": ", ".join(self_model.perceived_capabilities[-5:]),
            "phenomenological_report": format_phenomenological_report(drives, body_state),
            "strategy": winning_strategy.strategy_description if winning_strategy else "Padr√£o",
            "dynamic_style": generate_dynamic_style_directive(body_state, user_model),
            "internal_state_indicator": internal_state,
            "style_instruction": style_instruction,
            "working_memory": wm_context_str,

            # Blocos Din√¢micos Calculados Acima
            "history_block": history_block,
            "rules_block": rules_block,
            "advice_block": advice_block,
            "user_adapt_block": user_adapt_block,
            "task_block": task_block,
            "memory_context": memory_context,
            "tool_outputs": tool_str
        }

        # 8. INJE√á√ÉO NO TEMPLATE DO USU√ÅRIO
        try:
            # O template vem do JSON do usu√°rio (self.prompts.gth_rendering)
            rendering_prompt = self.prompts.gth_rendering.format(**prompt_vars)
        except KeyError as e:
            # Se o usu√°rio criar um template pedindo {variavel_inexistente}, cai aqui
            logger.warning(f"GTH: Template do usu√°rio pede vari√°vel desconhecida: {e}. Usando fallback.")
            rendering_prompt = f"Erro no template. Responda: {last_user_query}. Contexto: {memory_context}"
        except Exception as e:
            logger.error(f"GTH: Erro de formata√ß√£o: {e}")
            rendering_prompt = f"Responda: {last_user_query}"

        # Log para debug
        try:
            with open(PROMPT_LOG_FILE, "a", encoding="utf-8") as f:
                f.write(f"=== GTH PROMPT ===\n{rendering_prompt}\n=== END ===\n\n")
        except:
            pass

        steering_data = turn_context.get('active_steering')
        # 9. CHAMADA LLM
        try:
            model = self.llm_service.config.smart_model
            temp = effective_turn_context.get('temperature', self.llm_service.config.default_temperature)
            max_t = effective_turn_context.get('max_tokens', self.llm_service.config.max_tokens_output)

            # --- CORRE√á√ÉO DO LOG ---
            # Verifica qual provedor est√° realmente ativo no servi√ßo
            provider_display = model
            if hasattr(self.llm_service, 'inference_mode') and self.llm_service.inference_mode == 'vastai':
                provider_display = "Vast.AI (Qwen/SoulEngine)"

            logger.info(f"ü§ñ GTH: Chamando LLM [{provider_display}] com prompt de {len(rendering_prompt)} chars...")
            # -----------------------

            response = await self.llm_service.ainvoke(
                model,  # O LLMService vai ignorar isso se for VastAI
                rendering_prompt,
                temperature=temp,
                max_tokens=max_t,
                vector_data=steering_data
            )

            if response:
                import re
                # Remove tags <think>...</think> se o modelo vazou na resposta final
                response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()

            # ‚úÖ VALIDA√á√ÉO CR√çTICA
            if response is None:
                logger.error("‚ùå GTH: LLM retornou None!")
                return "Desculpe, houve um erro na gera√ß√£o da resposta."

            if not isinstance(response, str):
                logger.error(f"‚ùå GTH: LLM retornou tipo inesperado: {type(response)}")
                return f"Ol√°! Como posso ajudar com '{last_user_query}'?"

            final_response = response.strip()

            if not final_response:
                logger.warning("‚ö†Ô∏è GTH: LLM retornou string vazia!")
                return f"Entendi sua pergunta sobre '{last_user_query}'. Pode reformular?"

            logger.info(f"‚úÖ GTH: Resposta gerada com sucesso ({len(final_response)} chars).")
            return final_response

        except Exception as e:
            logger.error(f"‚ùå GTH: Erro cr√≠tico no LLM: {e}", exc_info=True)
            return f"Desculpe, houve um erro. Sobre '{last_user_query}', posso tentar de outra forma?"
